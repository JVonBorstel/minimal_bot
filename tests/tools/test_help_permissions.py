#!/usr/bin/env python3
"""Test help tool accessibility across different permission levels."""

import asyncio
import logging
import sys
import os
import pytest # Add pytest import

# Add the project root to Python path
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from tools.tool_executor import ToolExecutor
from config import get_config
from state_models import AppState
from user_auth.models import UserProfile

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
log = logging.getLogger("test_help_permissions")

# Permission levels to test
PERMISSION_LEVELS = ["ADMIN", "DEVELOPER", "STAKEHOLDER", "DEFAULT"]

@pytest.mark.parametrize("permission_level", PERMISSION_LEVELS)
@pytest.mark.asyncio
async def test_help_for_permission_level(permission_level):
    """Test help tool for a specific permission level."""
    print(f"\nüîç TESTING HELP FOR {permission_level} USER")
    print("=" * 50)
    
    try:
        config = get_config()
        app_state = AppState()
        
        # Create test user with specific permission level
        test_user = UserProfile(
            user_id=f"help_test_{permission_level.lower()}",
            display_name=f"Help Test {permission_level}",
            email=f"help_{permission_level.lower()}@test.com",
            assigned_role=permission_level
        )
        app_state.current_user = test_user
        
        print(f"‚úÖ Created {permission_level} user: {test_user.display_name}")
        
        executor = ToolExecutor(config)
        
        # Test basic help execution
        result = await executor.execute_tool("help", {}, app_state)
        
        print(f"üìä Response type: {type(result)}")
        
        # Validate response
        if not isinstance(result, dict):
            print(f"‚ùå Invalid response type for {permission_level}")
            return False
            
        if result.get('status') != 'SUCCESS':
            print(f"‚ùå Help execution failed for {permission_level}: {result}")
            return False
            
        help_data = result.get('data', {})
        if not isinstance(help_data, dict):
            print(f"‚ùå Invalid help data for {permission_level}")
            return False
            
        # Check basic structure
        required_keys = ['title', 'description', 'sections']
        for key in required_keys:
            if key not in help_data:
                print(f"‚ùå Missing key '{key}' for {permission_level}")
                return False
                
        print(f"‚úÖ {permission_level}: Help tool executed successfully")
        
        # Test help with topic
        topic_result = await executor.execute_tool("help", {"topic": "github"}, app_state)
        
        if not (isinstance(topic_result, dict) and topic_result.get('status') == 'SUCCESS'):
            print(f"‚ùå Topic help failed for {permission_level}")
            return False
            
        print(f"‚úÖ {permission_level}: Help with topic works")
        
        # Check content accessibility (all users should get help regardless of permission)
        sections = help_data.get('sections', [])
        if len(sections) < 3:
            print(f"‚ùå Insufficient help content for {permission_level}: {len(sections)} sections")
            return False
            
        print(f"‚úÖ {permission_level}: Good help content ({len(sections)} sections)")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Exception testing {permission_level}: {e}")
        log.error(f"Permission test exception for {permission_level}: {e}", exc_info=True)
        return False

@pytest.mark.asyncio
async def test_help_consistency_across_permissions():
    """Test that help content is consistent across different permission levels."""
    print("\nüîç TESTING HELP CONSISTENCY ACROSS PERMISSIONS")
    print("=" * 60)
    
    try:
        config = get_config()
        help_responses = {}
        
        # Get help response for each permission level
        for permission_level in PERMISSION_LEVELS:
            app_state = AppState()
            test_user = UserProfile(
                user_id=f"consistency_test_{permission_level.lower()}",
                display_name=f"Consistency Test {permission_level}",
                email=f"consistency_{permission_level.lower()}@test.com",
                assigned_role=permission_level
            )
            app_state.current_user = test_user
            
            executor = ToolExecutor(config)
            result = await executor.execute_tool("help", {}, app_state)
            
            if isinstance(result, dict) and result.get('status') == 'SUCCESS':
                help_responses[permission_level] = result['data']
                print(f"‚úÖ Got help response for {permission_level}")
            else:
                print(f"‚ùå Failed to get help response for {permission_level}")
                return False
        
        # Compare help responses (they should be essentially the same)
        base_response = help_responses[PERMISSION_LEVELS[0]]
        base_title = base_response.get('title', '')
        base_description = base_response.get('description', '')
        base_sections_count = len(base_response.get('sections', []))
        
        print(f"\nüìä Base response (from {PERMISSION_LEVELS[0]}):")
        print(f"   Title: '{base_title}'")
        print(f"   Description length: {len(base_description)} chars")
        print(f"   Sections count: {base_sections_count}")
        
        # Check consistency
        all_consistent = True
        for permission_level in PERMISSION_LEVELS[1:]:
            response = help_responses[permission_level]
            title = response.get('title', '')
            description = response.get('description', '')
            sections_count = len(response.get('sections', []))
            
            print(f"\nüìä Comparing {permission_level}:")
            
            # Title should be identical
            if title != base_title:
                print(f"‚ùå Title mismatch: '{title}' vs '{base_title}'")
                all_consistent = False
            else:
                print(f"‚úÖ Title matches")
            
            # Description should be identical
            if description != base_description:
                print(f"‚ùå Description mismatch (lengths: {len(description)} vs {len(base_description)})")
                all_consistent = False
            else:
                print(f"‚úÖ Description matches")
            
            # Sections count should be identical
            if sections_count != base_sections_count:
                print(f"‚ùå Sections count mismatch: {sections_count} vs {base_sections_count}")
                all_consistent = False
            else:
                print(f"‚úÖ Sections count matches ({sections_count})")
        
        if all_consistent:
            print("\n‚úÖ Help content is consistent across all permission levels")
        else:
            print("\n‚ùå Help content inconsistencies detected")
            
        return all_consistent
        
    except Exception as e:
        print(f"‚ùå Exception during consistency testing: {e}")
        log.error(f"Consistency test exception: {e}", exc_info=True)
        return False

@pytest.mark.asyncio
async def test_help_performance_by_permission():
    """Test help tool performance across different permission levels."""
    print("\nüîç TESTING HELP PERFORMANCE BY PERMISSION")
    print("=" * 60)
    
    try:
        import time
        config = get_config()
        performance_results = {}
        
        for permission_level in PERMISSION_LEVELS:
            app_state = AppState()
            test_user = UserProfile(
                user_id=f"perf_test_{permission_level.lower()}",
                display_name=f"Performance Test {permission_level}",
                email=f"perf_{permission_level.lower()}@test.com",
                assigned_role=permission_level
            )
            app_state.current_user = test_user
            
            executor = ToolExecutor(config)
            
            # Time the help execution
            start_time = time.time()
            result = await executor.execute_tool("help", {}, app_state)
            end_time = time.time()
            
            execution_time = end_time - start_time
            performance_results[permission_level] = execution_time
            
            if isinstance(result, dict) and result.get('status') == 'SUCCESS':
                print(f"‚úÖ {permission_level}: {execution_time:.3f}s")
            else:
                print(f"‚ùå {permission_level}: Failed ({execution_time:.3f}s)")
                return False
        
        # Check if all executions are reasonably fast
        max_acceptable_time = 2.0  # 2 seconds
        all_fast = True
        
        print(f"\nüìä Performance Summary (max acceptable: {max_acceptable_time}s):")
        for permission_level, exec_time in performance_results.items():
            status = "‚úÖ" if exec_time < max_acceptable_time else "‚ùå"
            print(f"   {status} {permission_level}: {exec_time:.3f}s")
            if exec_time >= max_acceptable_time:
                all_fast = False
        
        # Check for performance consistency (no permission should be much slower)
        times = list(performance_results.values())
        max_time = max(times)
        min_time = min(times)
        
        if max_time > 0 and min_time > 0:
            time_ratio = max_time / min_time
            print(f"\nüìä Performance consistency ratio: {time_ratio:.2f}x")
            
            if time_ratio > 3.0:  # No permission should be 3x slower than others
                print("‚ùå Performance inconsistency detected")
                all_fast = False
            else:
                print("‚úÖ Performance is consistent across permissions")
        else:
            print(f"\nüìä Performance consistency: All executions extremely fast (min: {min_time:.3f}s, max: {max_time:.3f}s)")
            print("‚úÖ Performance is excellent across all permissions")
        
        return all_fast
        
    except Exception as e:
        print(f"‚ùå Exception during performance testing: {e}")
        log.error(f"Performance test exception: {e}", exc_info=True)
        return False

@pytest.mark.asyncio
async def test_help_error_handling():
    """Test help tool error handling with invalid user scenarios."""
    print("\nüîç TESTING HELP ERROR HANDLING")
    print("=" * 50)
    
    try:
        config = get_config()
        
        # Test with None user (should handle gracefully)
        print("Testing with None user...")
        app_state = AppState()
        app_state.current_user = None
        
        executor = ToolExecutor(config)
        result = await executor.execute_tool("help", {}, app_state)
        
        # Should still work or fail gracefully
        if isinstance(result, dict):
            if result.get('status') == 'SUCCESS':
                print("‚úÖ Help works with None user")
            else:
                print("‚úÖ Help fails gracefully with None user")
        else:
            print("‚ö†Ô∏è  Unexpected response type with None user")
        
        # Test with invalid permission role
        print("Testing with invalid permission role...")
        app_state = AppState()
        test_user = UserProfile(
            user_id="invalid_role_test",
            display_name="Invalid Role Test",
            email="invalid@test.com",
            assigned_role="INVALID_ROLE"  # This should not be a valid role
        )
        app_state.current_user = test_user
        
        result = await executor.execute_tool("help", {}, app_state)
        
        # Should handle gracefully
        if isinstance(result, dict) and result.get('status') == 'SUCCESS':
            print("‚úÖ Help handles invalid role gracefully")
        else:
            print("‚úÖ Help fails gracefully with invalid role")
        
        return True
        
    except Exception as e:
        print(f"‚ö†Ô∏è  Expected behavior - help tool may not handle edge cases: {e}")
        # This is not a failure since error handling for edge cases is not critical
        return True

async def main():
    """Run all help permission tests."""
    print("ü§ñ STEP 1.15: HELP TOOL PERMISSION VALIDATION")
    print("=" * 70)
    
    print(f"üéØ TARGET: Validate help tool works for all permission levels")
    print(f"üìã Permission levels to test: {PERMISSION_LEVELS}")
    print()
    
    # Test 1: Individual permission levels
    individual_results = {}
    for permission_level in PERMISSION_LEVELS:
        individual_results[permission_level] = await test_help_for_permission_level(permission_level)
    
    # Test 2: Consistency across permissions
    consistency_success = await test_help_consistency_across_permissions()
    
    # Test 3: Performance by permission
    performance_success = await test_help_performance_by_permission()
    
    # Test 4: Error handling
    error_handling_success = await test_help_error_handling()
    
    # Summary
    print("\nüìä PERMISSION TEST SUMMARY")
    print("=" * 50)
    
    print("Individual Permission Tests:")
    all_individual_passed = True
    for permission_level, success in individual_results.items():
        status = "‚úÖ PASS" if success else "‚ùå FAIL"
        print(f"  {permission_level}: {status}")
        if not success:
            all_individual_passed = False
    
    print(f"\nCross-Permission Tests:")
    print(f"Consistency: {'‚úÖ PASS' if consistency_success else '‚ùå FAIL'}")
    print(f"Performance: {'‚úÖ PASS' if performance_success else '‚ùå FAIL'}")
    print(f"Error Handling: {'‚úÖ PASS' if error_handling_success else '‚ùå FAIL'}")
    
    overall_success = (all_individual_passed and consistency_success and 
                      performance_success and error_handling_success)
    
    if overall_success:
        print(f"\nüéâ ALL PERMISSION TESTS PASSED!")
        print(f"‚úÖ Help tool works correctly for all {len(PERMISSION_LEVELS)} permission levels")
        print(f"‚úÖ Content is consistent across permissions")
        print(f"‚úÖ Performance is acceptable for all permission levels")
    else:
        print(f"\n‚ùå PERMISSION TESTS FAILED!")
        print(f"‚ùå Help tool has permission-related issues")
    
    return overall_success

if __name__ == "__main__":
    asyncio.run(main()) 